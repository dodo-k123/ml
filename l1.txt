#---------------------------Decision Tree Algorithm

from sklearn import datasets
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier

iris = datasets.load_iris()
features = iris.data
tgt = iris.target
dectree = DecisionTreeClassifier(random_state = 0)
model = dectree.fit(features,tgt)
obs = [[3, 2, 3, 2]]
model.predict(obs)
model.predict_proba(obs)
tree.plot_tree(model)


# Create decision tree classifier object using entropy 
decisiontree_entropy = DecisionTreeClassifier(criterion='entropy', random_state=0) 
# Train model
 model_entropy = decisiontree_entropy.fit(features, target)


#---------------------------NaÃ¯ve Bayesian Classifier

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import GaussianNB

data = pd.read_csv("C:\\Users\\ITLAB3\\Social_Network_Ads.csv")
x= data.iloc[:,[2,3]].values
y= data.iloc[:,4].values
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=0)
sc=StandardScaler()
x_train=sc.fit_transform(x_train)
x_test=sc.transform(x_test)
classifier = GaussianNB()
classifier.fit(x_train,y_train)

from matplotlib.colors import ListedColormap
x_set,y_set=x_train,y_train
x1,x2=np.meshgrid(np.arange(start=x_set[:,0].min()-1,stop=x_set[:,0].max()+1,step=0.01),
                  np.arange(start=x_set[:,1].min()-1,stop=x_set[:,1].max()+1,step=0.01))
plt.contourf(x1,x2,classifier.predict(np.array([x1.ravel(),x2.ravel()]).T).reshape(x1.shape),alpha=0.75,
            cmap=ListedColormap(('red','green')))
plt.xlim(x1.min(),x1.max())
plt.ylim(x2.min(),x2.max())
for i,j in enumerate(np.unique(y_set)):
    plt.scatter(x_set[y_set==j,0],x_set[y_set==j,1],
               c=ListedColormap(('red','green'))(i),label=j)
plt.title('Naive Bayes (Training Set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()


y_pred=classifier.predict(x_test)
cm=confusion_matrix(y_test,y_pred)
new_obs=[[0.3, 0.9]]
classifier.predict(new_obs)
classifier.predict_proba(new_obs)


#---------------------------Support Vector Machine for Linear Data

from sklearn.svm import LinearSVC
from sklearn import datasets
from sklearn.preprocessing import StandardScaler
import numpy as np

iris = datasets.load_iris()
features = iris.data[:100,:2]
target = iris.target[:100]
scaler = StandardScaler()
features_standardized = scaler.fit_transform(features)
svc = LinearSVC(C=1.0)
model = svc.fit(features_standardized, target)

color = ["black" if c == 0 else "red" for c in target]
plt.scatter(features_standardized[:,0], features_standardized[:,1], c=color)
w = svc.coef_[0]
a = -w[0] / w[1]
xx = np.linspace(-2.5, 2.5)
yy = a * xx - (svc.intercept_[0]) / w[1]
plt.plot(xx, yy)
plt.axis("off"), plt.show();
new_observation = [[ -2, 3]]
svc.predict(new_observation)

#---------------------------K-Nearest Neighbors Algorithm 

from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn import datasets

iris = datasets.load_iris()
X = iris.data
y = iris.target
standardizer = StandardScaler()
X_std = standardizer.fit_transform(X)

# Train a KNN classifier with 5 neighbors
knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1).fit(X_std, y)

#Create two observations
new_observations = [[ 0.75, 0.75, 0.75, 0.75],[ 1, 1, 1, 1]]

# View probability each observation is one of three classes
knn.predict_proba(new_observations)
knn.predict(new_observations)

#---------------------------Nearest neighbors algorithm 

from sklearn import datasets 
from sklearn.neighbors import NearestNeighbors 
from sklearn.preprocessing import StandardScaler

iris = datasets.load_iris()
features = iris.data
standardizer = StandardScaler()
features_standardized = standardizer.fit_transform(features)

nearest_neighbors = NearestNeighbors(n_neighbors=2).fit(features_standardized)

distances, indices = nearest_neighbors.kneighbors([[ 1, 1, 1, 1]])
features_standardized[indices]

distances

nearestneighbors_euclidean = NearestNeighbors( n_neighbors=2, metric='euclidean').fit(features_standardized)

distances
features_standardized[indices]


#-------------------------Training a Multiclass Classifier - neural network
import numpy as np
from keras.datasets import reuters
from keras.utils.np_utils import to_categorical
from keras.preprocessing.text import Tokenizer
from keras import models
from keras import layers
np.random.seed(0)

number_of_features = 5000
(train_data, train_target_vector), (test_data, test_target_vector) = reuters.load_data(num_words=number_of_features)
tokenizer = Tokenizer(num_words=number_of_features)
train_features = tokenizer.sequences_to_matrix(train_data, mode='binary')
test_features = tokenizer.sequences_to_matrix(test_data, mode='binary')
train_target = to_categorical(train_target_vector)
test_target = to_categorical(test_target_vector)
# Start neural network
network = models.Sequential()
network.add(layers.Dense(units=100, activation='relu', input_shape=(number_of_features,)))
network.add(layers.Dense(units=100, activation='relu'))
network.add(layers.Dense(units=46, activation='softmax'))
network.compile(loss='categorical_crossentropy', # Cross-entropy
                optimizer='rmsprop', # Root Mean Square Propagation
                metrics=['accuracy']) # Accuracy performance metric
# Train neural network
history = network.fit(train_features, # Features
                      train_target, # Target vector
                      epochs=3, # Three epochs
                      #verbose=0, # No output
                      verbose=1, # for output
                      batch_size=100, # Number of observations per batch
                      validation_data=(test_features, test_target)) # Data to use for evaluation

#------------------------Train a binary classifier neural network
# Load libraries
import numpy as np
from keras.datasets import imdb
from keras.preprocessing.text import Tokenizer
from keras import models
from keras import layers

# Set random seed
np.random.seed(0)
# Set the number of features we want
number_of_features = 1000

# Load data and target vector from movie review data
(train_data, train_target), (test_data, test_target) = imdb.load_data(num_words=number_of_features)

# Convert movie review data to one-hot encoded feature matrix
tokenizer = Tokenizer(num_words=number_of_features)
train_features = tokenizer.sequences_to_matrix(train_data, mode='binary')
test_features = tokenizer.sequences_to_matrix(test_data, mode='binary')
# Start neural network
network = models.Sequential()

# Add fully connected layer with a ReLU activation function
network.add(layers.Dense(units=16, activation='relu', input_shape=(number_of_features,)))

# Add fully connected layer with a ReLU activation function
network.add(layers.Dense(units=16, activation='relu'))

# Add fully connected layer with a sigmoid activation function
network.add(layers.Dense(units=1, activation='sigmoid'))
# Compile neural network
network.compile(loss='binary_crossentropy', # Cross-entropy
                optimizer='rmsprop', # Root Mean Square Propagation
                metrics=['accuracy']) # Accuracy performance metric
# Train neural network
history = network.fit(train_features, # Features
                      train_target, # Target vector
                      epochs=5, # Number of epochs
                      verbose=1, # Print description after each epoch
                      batch_size=100, # Number of observations per batch
                      validation_data=(test_features, test_target))

#---------------------------SVM - Non-Linear
# Load the libraries
from sklearn.svm import SVC
from sklearn import datasets
from sklearn.preprocessing import StandardScaler
import numpy as np
np.random.seed(0)
features = np.random.randn(200, 2)
tgt_xor = np.logical_xor(features[:,0] > 0, features[:,1] > 0)
target = np.where(tgt_xor, 0, 1)
from matplotlib import pyplot as plt
from matplotlib.colors import ListedColormap
def plot_decision_regions(X, y, classifier):
    cmap = ListedColormap(("red", "green"))
    xx1, xx2 = np.meshgrid(np.arange(-3, 3, 0.02), np.arange(-3, 3, 0.02))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.1, cmap=cmap)
    for idx, cl in enumerate(np.unique(y)):
         plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],
         alpha=0.8, c=cmap(idx),
         marker="+", label=cl)
svc_linear = SVC(random_state = 0,kernel = 'linear',C=1.0)
svc_linear.fit(features, target)
plot_decision_regions(features, target, classifier = svc_linear)
plt.axis("off")
plt.show()

#Create a svm with radial basis function
svc = SVC(kernel = 'rbf', random_state = 0, gamma = 1, C = 1.0)
model = svc.fit(features,target)
plot_decision_regions(features, target, classifier = svc)
plt.axis("off")
plt.show()
